import React, { useRef, useEffect, useState, useCallback } from 'react';
import {
  Box, Typography, IconButton, Paper, Grid, LinearProgress, Chip, Button,
} from '@mui/material';
import {
  PlayArrow, Pause, Stop, VolumeUp, VolumeOff, Close,
} from '@mui/icons-material';

interface TrueRealTimePlayerProps {
  callId: string;
  phoneNumber: string;
  calledNumber?: string;
  hasClientFile: boolean;
  hasAgentFile: boolean;
  status: 'active' | 'completed';
  onClose?: () => void;
}

const TrueRealTimePlayer: React.FC<TrueRealTimePlayerProps> = ({
  callId, phoneNumber, calledNumber, hasClientFile, hasAgentFile, status, onClose,
}) => {
  const [isPlaying, setIsPlaying] = useState(false);
  const [volume, setVolume] = useState(0.8);
  const [isMuted, setIsMuted] = useState(false);
  const [streamInfo, setStreamInfo] = useState('Initialisation...');
  const [bytesReceived, setBytesReceived] = useState(0);
  const [progress, setProgress] = useState(0);
  const [callStatus, setCallStatus] = useState(status);
  const [debugInfo, setDebugInfo] = useState<string[]>([]);
  const [chunksReceived, setChunksReceived] = useState(0);
  const [chunksPlayed, setChunksPlayed] = useState(0);
  const [audioContextState, setAudioContextState] = useState('unknown');
  const [streamType, setStreamType] = useState<'in' | 'out' | 'both'>('both');
  
  const audioContextRef = useRef<AudioContext | null>(null);
  const sourceRef = useRef<AudioBufferSourceNode | null>(null);
  const gainNodeRef = useRef<GainNode | null>(null);
  const eventSourceRef = useRef<EventSource | null>(null);
  const audioBuffersRef = useRef<AudioBuffer[]>([]);
  const nextBufferTimeRef = useRef<number>(0);

  const addDebug = useCallback((message: string) => {
    const timestamp = new Date().toLocaleTimeString();
    setDebugInfo(prev => [...prev.slice(-4), `${timestamp}: ${message}`]);
  }, []);

  const testWavFile = async () => {
    try {
      await initAudioContext();
      if (!audioContextRef.current) {
        addDebug('‚ùå Impossible de cr√©er AudioContext pour WAV');
        return;
      }
      
      const ctx = audioContextRef.current;
      
      // Cr√©er un fichier WAV simple avec un ton de test
      const sampleRate = 8000;
      const duration = 1; // 1 seconde
      const numSamples = sampleRate * duration;
      
      // En-t√™te WAV
      const wavHeader = new ArrayBuffer(44);
      const headerView = new DataView(wavHeader);
      
      // RIFF header
      headerView.setUint32(0, 0x52494646, false); // "RIFF"
      headerView.setUint32(4, 36 + numSamples * 2, true); // file size
      headerView.setUint32(8, 0x57415645, false); // "WAVE"
      
      // fmt chunk
      headerView.setUint32(12, 0x666d7420, false); // "fmt "
      headerView.setUint32(16, 16, true); // chunk size
      headerView.setUint16(20, 1, true); // PCM
      headerView.setUint16(22, 1, true); // mono
      headerView.setUint32(24, sampleRate, true);
      headerView.setUint32(28, sampleRate * 2, true);
      headerView.setUint16(32, 2, true);
      headerView.setUint16(34, 16, true);
      
      // data chunk
      headerView.setUint32(36, 0x64617461, false); // "data"
      headerView.setUint32(40, numSamples * 2, true);
      
      // Donn√©es audio - ton de test 440Hz
      const audioData = new ArrayBuffer(numSamples * 2);
      const audioView = new DataView(audioData);
      
      for (let i = 0; i < numSamples; i++) {
        const sample = Math.sin(2 * Math.PI * 440 * i / sampleRate) * 16383;
        audioView.setInt16(i * 2, sample, true);
      }
      
      // Combiner header + data
      const wavFile = new ArrayBuffer(44 + numSamples * 2);
      const wavView = new Uint8Array(wavFile);
      wavView.set(new Uint8Array(wavHeader), 0);
      wavView.set(new Uint8Array(audioData), 44);
      
      addDebug(`üéµ WAV cr√©√©: ${wavFile.byteLength} bytes`);
      
      // D√©coder et jouer
      const audioBuffer = await ctx.decodeAudioData(wavFile.slice(0));
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(ctx.destination);
      source.start();
      
      addDebug(`üéµ WAV jou√©: ${audioBuffer.duration}s, ${audioBuffer.sampleRate}Hz`);
    } catch (error) {
      addDebug(`‚ùå Erreur test WAV: ${error.message}`);
    }
  };

  // Initialiser le contexte audio
  const initAudioContext = useCallback(async () => {
    try {
      // Si l'AudioContext est ferm√©, on le recr√©e compl√®tement
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        addDebug('üÜï Cr√©ation nouveau AudioContext');
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        gainNodeRef.current = audioContextRef.current.createGain();
        gainNodeRef.current.connect(audioContextRef.current.destination);
        gainNodeRef.current.gain.value = volume;
        addDebug(`AudioContext cr√©√©, volume: ${volume}`);
      }

      // Seulement reprendre si suspendu (pas ferm√©)
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
        addDebug(`AudioContext repris: ${audioContextRef.current.state}`);
      }
      
      setAudioContextState(audioContextRef.current.state);
      addDebug(`‚úÖ AudioContext final state: ${audioContextRef.current.state}`);
      
      return audioContextRef.current.state === 'running';
    } catch (error) {
      addDebug(`‚ùå Erreur AudioContext: ${error}`);
      // En cas d'erreur, forcer une nouvelle cr√©ation
      try {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        gainNodeRef.current = audioContextRef.current.createGain();
        gainNodeRef.current.connect(audioContextRef.current.destination);
        gainNodeRef.current.gain.value = volume;
        addDebug('üîÑ AudioContext recr√©√© apr√®s erreur');
        setAudioContextState(audioContextRef.current.state);
        return audioContextRef.current.state === 'running';
      } catch (error2) {
        addDebug(`‚ùå √âchec total AudioContext: ${error2}`);
        return false;
      }
    }
  }, [volume, addDebug]);

  // Convertir les donn√©es base64 en AudioBuffer
  const decodeAudioData = useCallback(async (base64Data: string) => {
    if (!audioContextRef.current) {
      addDebug('‚ùå Pas d\'AudioContext pour d√©coder');
      return null;
    }

    try {
      const binaryString = atob(base64Data);
      const arrayBuffer = new ArrayBuffer(binaryString.length);
      const uint8Array = new Uint8Array(arrayBuffer);
      
      for (let i = 0; i < binaryString.length; i++) {
        uint8Array[i] = binaryString.charCodeAt(i);
      }

      const audioBuffer = await audioContextRef.current.decodeAudioData(arrayBuffer);
      addDebug(`‚úÖ D√©cod√©: ${audioBuffer.duration.toFixed(2)}s, ${audioBuffer.numberOfChannels}ch`);
      return audioBuffer;
    } catch (error) {
      addDebug(`‚ùå Erreur d√©codage: ${error}`);
      console.error('Erreur d√©codage audio:', error);
      return null;
    }
  }, [addDebug]);

  // Jouer un buffer audio
  const playAudioBuffer = useCallback((audioBuffer: AudioBuffer) => {
    if (!audioContextRef.current || !gainNodeRef.current) {
      addDebug('‚ùå Pas d\'AudioContext/GainNode pour jouer');
      return;
    }

    const source = audioContextRef.current.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(gainNodeRef.current);
    
    // Timing s√©quentiel strict pour √©viter l'√©cho
    const currentTime = audioContextRef.current.currentTime;
    const startTime = Math.max(currentTime + 0.01, nextBufferTimeRef.current); // +10ms buffer minimum
    source.start(startTime);
    
    // Pas d'overlap - chunks s√©quentiels stricts
    nextBufferTimeRef.current = startTime + audioBuffer.duration;
    setChunksPlayed(prev => prev + 1);
    addDebug(`üéµ Jou√©: chunk ${chunksPlayed + 1}, startTime: ${startTime.toFixed(3)}s, next: ${nextBufferTimeRef.current.toFixed(3)}s`);
  }, [addDebug, chunksPlayed]);

  // D√©marrer le streaming temps r√©el
  const startRealTimeStream = useCallback(async () => {
    // FORCER la fermeture de toutes les connexions pr√©c√©dentes
    if (eventSourceRef.current) {
      console.log('üîå Fermeture connexion pr√©c√©dente...');
      
      // Attendre la fermeture R√âELLE avec un Promise
      await new Promise<void>((resolve) => {
        const currentEventSource = eventSourceRef.current!;
        
        // √âcouter l'√©v√©nement de fermeture
        const onClose = () => {
          console.log('‚úÖ Connexion pr√©c√©dente ferm√©e');
          resolve();
        };
        
        // Fermer et attendre
        currentEventSource.addEventListener('error', onClose);
        currentEventSource.close();
        
        // Fallback au cas o√π l'√©v√©nement ne se d√©clenche pas
        setTimeout(() => {
          console.log('‚è∞ Timeout fermeture connexion');
          resolve();
        }, 500);
      });
      
      eventSourceRef.current = null;
    }

    // R√©initialiser le buffer timing pour √©viter l'√©cho
    nextBufferTimeRef.current = 0;
    setChunksPlayed(0);
    
    await initAudioContext();

    // Utiliser le stream mix√© pour entendre les deux voix
    const streamUrl = `http://ai.intelios.us:5002/api/calls/${callId}/stream-sln/mixed`;
    
    setStreamInfo(`Connexion au streaming ${streamType}...`);
    
    eventSourceRef.current = new EventSource(streamUrl);

    eventSourceRef.current.addEventListener('connected', (event) => {
      const data = JSON.parse(event.data);
      setStreamInfo(`üîó Connect√©: ${data.message}`);
      addDebug('üîó Stream connect√©, FOR√áAGE lecture √† true');
      setIsPlaying(true); // Force la lecture √† true
      console.log('Stream connect√©:', data);
    });

    eventSourceRef.current.addEventListener('audio-chunk', async (event) => {
      const data = JSON.parse(event.data);
      setBytesReceived(prev => prev + data.chunkSize);
      setProgress((data.offset / data.fileSize) * 100);
      setCallStatus(data.callStatus);
      setChunksReceived(prev => prev + 1);

      addDebug(`üì¶ Chunk ${chunksReceived + 1}: ${data.chunkSize} bytes, playing: ${isPlaying}`);

      // D√©coder et jouer le chunk audio
      const audioBuffer = await decodeAudioData(data.data);
      
      // Jouer directement si AudioContext est pr√™t
      if (audioBuffer && audioContextRef.current?.state === 'running') {
        playAudioBuffer(audioBuffer);
      } else if (!audioBuffer) {
        addDebug('‚ùå √âchec d√©codage chunk');
      } else if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        addDebug('‚ö†Ô∏è AudioContext ferm√©, chunk mis en attente');
        // On pourrait stocker les chunks en attente ici
      } else {
        addDebug(`‚è∏Ô∏è AudioContext ${audioContextRef.current?.state}, chunk ignor√©`);
      }

      setStreamInfo(`üéµ Lecture: ${(data.offset / 1024 / 1024).toFixed(1)} MB / ${(data.fileSize / 1024 / 1024).toFixed(1)} MB`);
    });

    eventSourceRef.current.addEventListener('status', (event) => {
      const data = JSON.parse(event.data);
      setProgress(data.progress);
      setCallStatus(data.callStatus);
      
      if (data.callStatus === 'completed' && data.progress >= 100) {
        setStreamInfo('‚úÖ Appel termin√© - Lecture compl√®te');
      } else if (data.callStatus === 'active') {
        setStreamInfo(`üî¥ En cours: ${data.progress.toFixed(1)}%`);
      }
    });

    eventSourceRef.current.addEventListener('waiting', (event) => {
      const data = JSON.parse(event.data);
      setStreamInfo(`‚è≥ ${data.message}`);
    });

    eventSourceRef.current.addEventListener('error', (event) => {
      console.error('Erreur streaming:', event);
      setStreamInfo('‚ùå Erreur de connexion');
    });

    eventSourceRef.current.onerror = (error) => {
      console.error('EventSource error:', error);
      setStreamInfo('‚ùå Connexion perdue - Reconnexion...');
      
      // Reconnexion automatique apr√®s 2 secondes
      setTimeout(() => {
        if (isPlaying) {
          startRealTimeStream();
        }
      }, 2000);
    };
  }, [callId, hasClientFile, isPlaying, initAudioContext, decodeAudioData, playAudioBuffer]);

  // Contr√¥les de lecture
  const handlePlay = async () => {
    addDebug('üé¨ Bouton Play cliqu√© - FOR√áAGE isPlaying = true');
    setIsPlaying(true);
    await startRealTimeStream();
    addDebug('‚úÖ Streaming d√©marr√©, isPlaying devrait √™tre true');
    // Double v√©rification
    setTimeout(() => {
      addDebug(`üîç V√©rification: isPlaying = ${isPlaying}`);
    }, 100);
  };

  const handlePause = async () => {
    console.log('‚è∏Ô∏è Pause demand√©e - fermeture connexion');
    setIsPlaying(false);
    
    // Fermeture propre de la connexion avec attente
    if (eventSourceRef.current) {
      await new Promise<void>((resolve) => {
        const currentEventSource = eventSourceRef.current!;
        
        const onClose = () => {
          console.log('‚úÖ Connexion ferm√©e sur pause');
          resolve();
        };
        
        currentEventSource.addEventListener('error', onClose);
        currentEventSource.close();
        
        setTimeout(() => {
          console.log('‚è∞ Timeout fermeture pause');
          resolve();
        }, 300);
      });
      
      eventSourceRef.current = null;
    }
    
    if (sourceRef.current) {
      sourceRef.current.stop();
    }
    
    // R√©initialiser le timing pour √©viter l'√©cho
    nextBufferTimeRef.current = 0;
    addDebug('‚è∏Ô∏è Pause + fermeture connexion confirm√©e');
  };

  const handleStop = () => {
    setIsPlaying(false);
    setBytesReceived(0);
    setProgress(0);
    if (eventSourceRef.current) {
      eventSourceRef.current.close();
    }
    if (sourceRef.current) {
      sourceRef.current.stop();
    }
    setStreamInfo('Arr√™t√©');
  };

  const handleVolumeChange = (newVolume: number) => {
    setVolume(newVolume);
    if (gainNodeRef.current) {
      gainNodeRef.current.gain.value = newVolume;
    }
  };

  const toggleMute = () => {
    setIsMuted(!isMuted);
    if (gainNodeRef.current) {
      gainNodeRef.current.gain.value = isMuted ? volume : 0;
    }
  };

  // D√©marrage automatique
  useEffect(() => {
    let mounted = true;
    const autoStart = async () => {
      if (!mounted) return;
      addDebug('üöÄ Composant mont√©, d√©marrage auto');
      setIsPlaying(true);
      await startRealTimeStream();
      if (mounted) addDebug('‚úÖ Streaming auto-d√©marr√©');
    };
    autoStart();
    return () => { mounted = false; };
  }, []); // Pas de d√©pendances

  // Nettoyage
  useEffect(() => {
    return () => {
      if (eventSourceRef.current) {
        eventSourceRef.current.close();
      }
      if (sourceRef.current) {
        sourceRef.current.stop();
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  return (
    <Paper elevation={4} sx={{ p: 3, mt: 2, backgroundColor: '#f5f5f5' }}>
      <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 2 }}>
        <Typography variant="h6" color="primary">
          üéß Lecteur Temps R√©el - {phoneNumber}
          {calledNumber && ` ‚Üí ${calledNumber}`}
        </Typography>
        <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
          <Chip 
            label={callStatus === 'active' ? 'EN COURS' : 'TERMIN√â'} 
            color={callStatus === 'active' ? 'success' : 'default'} 
            size="small"
          />
          {onClose && (
            <IconButton onClick={onClose} size="small">
              <Close />
            </IconButton>
          )}
        </Box>
      </Box>

      {/* Informations de streaming */}
      <Box sx={{ mb: 2 }}>
        <Typography variant="body2" color="text.secondary">
          {streamInfo}
        </Typography>
        <LinearProgress 
          variant="determinate" 
          value={progress} 
          sx={{ mt: 1, height: 8, borderRadius: 4 }}
        />
        <Typography variant="caption" color="text.secondary">
          {(bytesReceived / 1024 / 1024).toFixed(1)} MB re√ßus - {progress.toFixed(1)}%
        </Typography>
      </Box>

      {/* Contr√¥les */}
      <Grid container spacing={2} alignItems="center">
        <Grid item>
          <IconButton
            onClick={() => {
              addDebug(`üéÆ Bouton ${isPlaying ? 'Pause' : 'Play'} cliqu√© (isPlaying: ${isPlaying})`);
              if (isPlaying) {
                handlePause();
              } else {
                handlePlay();
              }
            }}
            color="primary"
            size="large"
            disabled={!hasClientFile && !hasAgentFile}
          >
            {isPlaying ? <Pause /> : <PlayArrow />}
          </IconButton>
        </Grid>
        
        <Grid item>
          <IconButton onClick={handleStop} color="secondary">
            <Stop />
          </IconButton>
        </Grid>

        {audioContextState === 'closed' && (
          <Grid item>
            <Button 
              variant="contained" 
              color="secondary" 
              onClick={async () => {
                addDebug('üî¥ Bouton D√©marrer Audio cliqu√©');
                const success = await initAudioContext();
                if (success) {
                  setIsPlaying(true);
                  addDebug('‚úÖ Audio d√©marr√© + lecture activ√©e');
                } else {
                  addDebug('‚ùå √âchec d√©marrage audio');
                }
              }}
              startIcon={<VolumeUp />}
              size="small"
            >
              D√©marrer Audio
            </Button>
          </Grid>
        )}

        <Grid item>
          <Button 
            variant="outlined" 
            color="warning"
            onClick={async () => {
              // Test son simple
              const ctx = new AudioContext();
              const oscillator = ctx.createOscillator();
              const gain = ctx.createGain();
              
              oscillator.connect(gain);
              gain.connect(ctx.destination);
              
              oscillator.frequency.value = 440; // La note
              gain.gain.value = 0.1;
              
              oscillator.start();
              setTimeout(() => oscillator.stop(), 200);
              
              addDebug(`üîä Test son: AudioContext.state = ${ctx.state}`);
            }}
            size="small"
          >
            üîä Test Son
          </Button>
        </Grid>
        
        {/* Test WAV */}
        <Grid item>
          <Button
            variant="contained"
            color="secondary"
            onClick={testWavFile}
            size="small"
          >
            üéµ Test WAV
          </Button>
        </Grid>

        {/* Info: Stream mix√© */}
        <Grid item>
          <Typography variant="caption" color="primary">
            üé≠ Conversation compl√®te (Client + Agent)
          </Typography>
        </Grid>

        <Grid item xs>
          <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
            <IconButton onClick={toggleMute} size="small">
              {isMuted ? <VolumeOff /> : <VolumeUp />}
            </IconButton>
            <input
              type="range"
              min="0"
              max="1"
              step="0.1"
              value={isMuted ? 0 : volume}
              onChange={(e) => handleVolumeChange(parseFloat(e.target.value))}
              style={{ width: '100px' }}
            />
            <Typography variant="caption">
              {Math.round((isMuted ? 0 : volume) * 100)}%
            </Typography>
          </Box>
        </Grid>
      </Grid>

      {/* Debug Info */}
      <Box sx={{ mt: 2, p: 2, backgroundColor: '#f5f5f5', borderRadius: 1 }}>
        <Typography variant="caption" fontWeight="bold">
          üîß Debug Audio:
        </Typography>
        <Box sx={{ mt: 1 }}>
          <Typography variant="caption" display="block">
            AudioContext: {audioContextState} | Chunks: {chunksReceived} re√ßus, {chunksPlayed} jou√©s | Volume: {Math.round(volume * 100)}%
          </Typography>
          {debugInfo.map((info, i) => (
            <Typography key={i} variant="caption" display="block" color="text.secondary" sx={{ fontFamily: 'monospace' }}>
              {info}
            </Typography>
          ))}
        </Box>
      </Box>

      {/* Instructions */}
      <Box sx={{ mt: 2, p: 2, backgroundColor: '#e3f2fd', borderRadius: 1 }}>
        <Typography variant="caption" color="primary">
          üí° <strong>Temps R√©el:</strong> Ce lecteur re√ßoit l'audio au fur et √† mesure qu'il est g√©n√©r√©.
          {callStatus === 'active' && ' L\'appel est en cours, vous entendez en direct !'}
        </Typography>
      </Box>
    </Paper>
  );
};

export default TrueRealTimePlayer;
